{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sriva\\Desktop\\digital-human\\.env\n"
     ]
    },
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m load_dotenv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dot_env_path))\n\u001b[0;32m     11\u001b[0m openai_api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m speech_file_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;18m__file__\u001b[39m)\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeech.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     16\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39maudio\u001b[38;5;241m.\u001b[39mspeech\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     17\u001b[0m   model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtts-1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m   voice\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malloy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m   \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToday is a wonderful day to build something people love!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     20\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\sriva\\Desktop\\digital-human\\venv\\lib\\site-packages\\openai\\_client.py:104\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    102\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m     )\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "dot_env_path = r'C:\\Users\\sriva\\Desktop\\digital-human\\.env'\n",
    "print(dot_env_path)\n",
    "\n",
    "load_dotenv(os.path.join(dot_env_path))\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "speech_file_path = Path(__file__).parent / \"speech.mp3\"\n",
    "response = client.audio.speech.create(\n",
    "  model=\"tts-1\",\n",
    "  voice=\"alloy\",\n",
    "  input=\"Today is a wonderful day to build something people love!\"\n",
    ")\n",
    "\n",
    "response.stream_to_file(speech_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import win32com.client\n",
    "import pythoncom\n",
    "\n",
    "text = \"Hello I’m a virtual entity designed to interact with humans in a conversational and engaging manner. I can understand natural language, generate responses, and even adapt to the context of our conversation. My visual representation is created using computer-generated imagery (CGI), so I can have a face and body to make our interactions feel more human-like.\"\n",
    "\n",
    "pythoncom.CoInitialize()\n",
    "speaker = win32com.client.Dispatch(\"SAPI.SpVoice\")\n",
    "speaker.Speak(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "\n",
    "# Initialize the TTS engine\n",
    "tts_engine = pyttsx3.init()\n",
    "\n",
    "# Set the speech rate (words per minute)\n",
    "tts_engine.setProperty('rate', 165)  # Default is typically around 200\n",
    "\n",
    "# Set the volume (0.0 to 1.0)\n",
    "tts_engine.setProperty('volume', 0.9)  # Default is 1.0\n",
    "\n",
    "# Choose a voice (index 0 for default, or use a specific language)\n",
    "tts_engine.setProperty('voice', 'HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0')  # Change index for other voices\n",
    "\n",
    "# Convert text to speech\n",
    "text = \"Hm Hello I’m a virtual entity designed to interact with humans in a conversational and engaging manner. I can understand natural language, generate responses, and even adapt to the context of our conversation. My visual representation is created using computer-generated imagery (CGI), so I can have a face and body to make our interactions feel more human-like.\"\n",
    "\n",
    "tts_engine.say(text)\n",
    "tts_engine.runAndWait()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## edge-tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dict = {\n",
    " 'English-Jenny (Female)': 'en-US-JennyNeural',\n",
    " 'English-Guy (Male)': 'en-US-GuyNeural',\n",
    " 'English-Ana (Female)': 'en-US-AnaNeural',\n",
    " 'English-Aria (Female)': 'en-US-AriaNeural',\n",
    " 'English-Christopher (Male)': 'en-US-ChristopherNeural',\n",
    " 'English-Eric (Male)': 'en-US-EricNeural',\n",
    " 'English-Michelle (Female)': 'en-US-MichelleNeural',\n",
    " 'English-Roger (Male)': 'en-US-RogerNeural',\n",
    " 'English (Australia)-Natasha- (Female)': 'en-AU-NatashaNeural',\n",
    " 'English (Australia)-William- (Male)': 'en-AU-WilliamNeural',\n",
    " 'English (Canada)-Clara- (Female)': 'en-CA-ClaraNeural',\n",
    " 'English (Canada)-Liam- (Male)': 'en-CA-LiamNeural',\n",
    " 'English (UK)-Libby- (Female)': 'en-GB-LibbyNeural',\n",
    " 'English (UK)-Maisie- (Female)': 'en-GB-MaisieNeural',\n",
    " 'English (UK)-Ryan- (Male)': 'en-GB-RyanNeural',\n",
    " 'English (UK)-Sonia- (Female)': 'en-GB-SoniaNeural',\n",
    " 'English (UK)-Thomas- (Male)': 'en-GB-ThomasNeural',\n",
    " 'English (Hong Kong)-Sam- (Male)': 'en-HK-SamNeural',\n",
    " 'English (Hong Kong)-Yan- (Female)': 'en-HK-YanNeural',\n",
    " 'English (Ireland)-Connor- (Male)': 'en-IE-ConnorNeural',\n",
    " 'English (Ireland)-Emily- (Female)': 'en-IE-EmilyNeural',\n",
    " 'English (India)-Neerja- (Female)': 'en-IN-NeerjaNeural',\n",
    " 'English (India)-Prabhat- (Male)': 'en-IN-PrabhatNeural',\n",
    " 'English (Kenya)-Asilia- (Female)': 'en-KE-AsiliaNeural',\n",
    " 'English (Kenya)-Chilemba- (Male)': 'en-KE-ChilembaNeural',\n",
    " 'English (Nigeria)-Abeo- (Male)': 'en-NG-AbeoNeural',\n",
    " 'English (Nigeria)-Ezinne- (Female)': 'en-NG-EzinneNeural',\n",
    " 'English (New Zealand)-Mitchell- (Male)': 'en-NZ-MitchellNeural',\n",
    " 'English (Philippines)-James- (Male)': 'en-PH-JamesNeural',\n",
    " 'English (Philippines)-Rosa- (Female)': 'en-PH-RosaNeural',\n",
    " 'English (Singapore)-Luna- (Female)': 'en-SG-LunaNeural',\n",
    " 'English (Singapore)-Wayne- (Male)': 'en-SG-WayneNeural',\n",
    " 'English (Tanzania)-Elimu- (Male)': 'en-TZ-ElimuNeural',\n",
    " 'English (Tanzania)-Imani- (Female)': 'en-TZ-ImaniNeural',\n",
    " 'English (South Africa)-Leah- (Female)': 'en-ZA-LeahNeural',\n",
    " 'English (South Africa)-Luke- (Male)': 'en-ZA-LukeNeural'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'stream_tts' has no attribute 'amain'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m VOICE \u001b[38;5;241m=\u001b[39m language_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish (India)-Neerja- (Female)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men-IN-NeerjaNeural\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men-GB-SoniaNeural\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m OUTPUT_FILE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m stream_tts\u001b[38;5;241m.\u001b[39mamain(TEXT, VOICE, OUTPUT_FILE)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'stream_tts' has no attribute 'amain'"
     ]
    }
   ],
   "source": [
    "import stream_tts\n",
    "\n",
    "TEXT = \"Hello my brother\"\n",
    "VOICE = language_dict.get(\"English (India)-Neerja- (Female)': 'en-IN-NeerjaNeural\",\"en-GB-SoniaNeural\")\n",
    "OUTPUT_FILE = \"test.mp3\"\n",
    "\n",
    "\n",
    "await stream_tts.amain(TEXT, VOICE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sriva\\AppData\\Local\\Temp\\ipykernel_18388\\2802625659.py:9: RuntimeWarning: coroutine 'generate_audio' was never awaited\n",
      "  stream_tts.generate_audio(text, voice, output_file)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
